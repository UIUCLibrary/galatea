"""Merge data from one source with another.

Added in version 0.4.0.

"""

import functools
import logging
import pathlib
import re
import csv
import sys
if sys.version_info >= (3, 11):
    import tomllib
else:
    import tomli as tomllib
import typing
from typing import (
    List,
    Dict,
    TextIO,
    Callable,
    BinaryIO,
    Type,
    Union,
    Optional,
    Iterable,
)
import dataclasses
import requests
from xml.etree import ElementTree as ET

from galatea import tsv

__all__ = [
    "generate_mapping_file_for_tsv",
    "merge_from_getmarc",
    "BadMappingFileError",
]

from galatea.tsv import TableRow

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class BadMappingDataError(Exception):
    """Exception raised when a mapping data is malformed or invalid."""

    def __init__(self, *args, details: Optional[str] = None) -> None:
        super().__init__(*args)
        self.details = details


class BadMappingFileError(Exception):
    """Exception raised when a mapping data in a file malformed or invalid."""

    def __init__(
        self, source_file: pathlib.Path, *args, details: Optional[str] = None
    ) -> None:
        """Initialize a new BadMappingFileError exception.

        Args:
            source_file: source file containing the error
            *args: standard exception args
            details: any details about error
        """
        super().__init__(*args)
        self.source = source_file
        self.details = details

    def __str__(self) -> str:
        """Print string."""
        if self.details:
            return (
                f"Issue with mapping file: {self.source}. "
                f"Details: {self.details}."
            )
        return f"Issue with mapping file: {self.source}"


@dataclasses.dataclass
class MappingConfig:
    key: str
    matching_keys: List[str]
    delimiter: str
    existing_data: str


def get_keys_from_tsv_fp(fp: TextIO) -> List[str]:
    """Get the keys from a TSV file.

    Args:
        fp: file pointer to the TSV file.

    Returns: list of keys (headings) in the TSV file.

    """

    def _iter_keys_from_fp(fp: TextIO) -> TableRow[Dict[str, str]]:
        return next(iter(tsv.iter_tsv_fp(fp, dialect="excel-tab")))

    headings = []
    starting = fp.tell()
    try:
        fp.seek(0)
        for key in _iter_keys_from_fp(fp).entry.keys():
            if not key:
                continue
            if key in headings:
                continue
            headings.append(key)
        return headings
    finally:
        fp.seek(starting)


def get_keys_from_tsv(
    tsv_file: pathlib.Path,
    strategy: Callable[[TextIO], List[str]] = get_keys_from_tsv_fp,
) -> List[str]:
    with tsv_file.open("r") as f:
        return strategy(f)


def generate_mapping_toml_file_for_tsv_fp(
    source_file_name: str, headings: List[str], fp: TextIO
) -> None:
    """Generate a mapping file for a TSV file.

    Args:
        source_file_name: source file name, used in the mapping file
        headings: list of headings in the TSV file.
        fp: file pointer to write the mapping file to.

    """
    boilerplate = f"""# ====================================================
# Mapping file for "{source_file_name}"
# ====================================================

# This config file is generated by galatea to help you map data from another 
# source (such as GetMARC/ALMA) into the fields of your tsv file.

[mappings]

# the value set in identifier_key is the key that use look up a matching record
identifier_key = "Bibliographic Identifier"

# Within this [mappings] section, there is a list of sub-elements 
# entitled [[mapping]]. This is where you do your mapping. The key 
# corresponds to the key in the tsv file, and the matching_keys is a list of 
# keys that you want to pull data from. 

# For example. If you want to pull data from the 264$c field of the MARC 
# record into the "Date" column, your mapping file would look like this:
 
# [[mapping]]
# key = "Date"
# matching_marc_fields = [
#   "264$c"
# ]

# This will map the 264$c field of the MARC record into the "Date" column of 
# your tsv file.

# If you do not want to map a field, you can leave the matching_keys empty or 
# completely remove the [[mapping]] section.

"""
    fp.write(boilerplate)
    for i, key in enumerate(headings):
        fp.write("[[mapping]]\n")
        fp.write(f'key = "{key}"\n')
        fp.write("matching_marc_fields = []\n")
        fp.write('delimiter = "||"\n')
        fp.write('existing_data = "keep"\n')
        fp.write("\n")
    fp.write(
        """
# This file uses the TOML format. If you should need more information about the 
# syntax, see: https://toml.io/en/
""".lstrip()
    )


def generate_mapping_file_for_tsv(
    tsv_file: pathlib.Path,
    output_file: pathlib.Path,
    headers_reading_strategy: Callable[
        [pathlib.Path], List[str]
    ] = get_keys_from_tsv,
    writing_strategy: Callable[
        [str, List[str], TextIO], None
    ] = generate_mapping_toml_file_for_tsv_fp,
) -> None:
    """Generate a mapping file for a TSV file.

    The mapping file will contain the field names and what index they are mapped from
    """
    with output_file.open("w") as f:
        writing_strategy(tsv_file.name, headers_reading_strategy(tsv_file), f)
    print(f"Wrote mapping file to {output_file.absolute()}")


def get_matching_marc_data(
    mmsid: str,
    get_marc_server: str,
    request_strategy: Callable[[str], requests.Response] = requests.get,
) -> ET.Element:
    result = request_strategy(f"{get_marc_server}/api/record?mms_id={mmsid}")
    return ET.fromstring(result.text)


MARC_REGEX = r"^(?P<datafield>\d{3})((\$?(?P<subfield>[a-zA-Z0-9]{1}))?$)"


def get_xpath(datafield: str, subfield: Union[str, None], prefix: str) -> str:
    if not subfield:
        return f".//{prefix}:datafield[@tag='{datafield}']"
    return f".//{prefix}:datafield[@tag='{datafield}']/{prefix}:subfield[@code='{subfield}']"

# ===================================================================
# Validation functions for mapping entries
def validate_is_list_of_strings(
    entry: Dict[str, Union[str, List[str]]], key: str
) -> Optional[str]:
    value = entry.get(key)
    if not isinstance(value, list):
        return f"'{key}' should be a list of strings"

    for item in value:
        if not isinstance(item, str):
            return f"'{key}' should be a list of strings, but found {type(item).__name__}"
    return None


def validate_is_not_list(
    entry: Dict[str, Union[str, List[str]]], key: str
) -> Optional[str]:
    value = entry.get(key)
    if isinstance(value, list):
        return f"'{key}' should be a string, not a list"
    return None


def validate_is_string(
    entry: Dict[str, Union[str, List[str]]], key: str
) -> Optional[str]:
    value = entry.get(key)
    if not isinstance(value, str):
        return f"'{key}' should be a string"
    return None


def validate_limited_to_values(
    entry: Dict[str, Union[str, List[str]]],
    key: str,
    allowed_values: List[str],
) -> Optional[str]:
    value = entry.get(key)
    if value not in allowed_values:
        return (
            f"'{key}' should be one of {allowed_values}, but found '{value}'"
        )
    return None

# ===================================================================

DEFAULT_MARC_MAPPING_VALIDATIONS: List[
    Callable[[Dict[str, Union[str, List[str]]]], Optional[str]]
] = [
    functools.partial(validate_is_not_list, key="key"),
    functools.partial(validate_is_list_of_strings, key="matching_marc_fields"),
    functools.partial(validate_is_string, key="delimiter"),
    functools.partial(
        validate_limited_to_values,
        key="existing_data",
        allowed_values=["keep", "replace", "append"],
    ),
]


def map_marc_mapping_to_mapping_config(
    entry: Dict[str, Union[str, List[str]]],
    validations: Optional[
        List[Callable[[Dict[str, Union[str, List[str]]]], Optional[str]]]
    ] = None,
) -> MappingConfig:
    errors: List[str] = []
    for check in validations or DEFAULT_MARC_MAPPING_VALIDATIONS:
        found_issue = check(entry)
        if found_issue:
            errors.append(found_issue)

    if errors:
        raise BadMappingDataError(
            "Malformed mapping file: " + ", ".join(errors)
        )

    return MappingConfig(
        key=typing.cast(str, entry["key"]),
        matching_keys=(
            typing.cast(List[str], entry.get("matching_marc_fields", []))),
        delimiter=typing.cast(str, entry.get("delimiter", "||")),
        existing_data=typing.cast(str, entry.get("existing_data", "keep")),
    )


def read_mapping_toml_data(
    mapping_file_fp: BinaryIO,
    mapping_to_config_strategy: Optional[
        Callable[[Dict[str, Union[str, List[str]]]], MappingConfig]
    ] = None,
) -> Dict[str, MappingConfig]:
    starting = mapping_file_fp.tell()
    mapping = {}
    try:
        mapping_file_fp.seek(0)
        mapping_data = tomllib.load(mapping_file_fp)
        for mapping_value in mapping_data["mapping"]:
            try:
                if mapping_to_config_strategy is None:
                    mapping[mapping_value["key"]] = MappingConfig(
                        **mapping_value
                    )
                else:
                    mapping[mapping_value["key"]] = mapping_to_config_strategy(
                        mapping_value
                    )
            except TypeError:
                raise BadMappingDataError("Malformed mapping file")
        return mapping
    except tomllib.TOMLDecodeError as toml_error:
        raise BadMappingDataError(details=str(toml_error)) from toml_error
    finally:
        mapping_file_fp.seek(starting)


def read_mapping_file(
    mapping_file: pathlib.Path,
    mapping_strategy: Callable[
        [BinaryIO], Dict[str, MappingConfig]
    ] = read_mapping_toml_data,
) -> Dict[str, MappingConfig]:
    with mapping_file.open("rb") as fp:
        return mapping_strategy(fp)


def get_identifier_key_fp(mapping_file_fp: BinaryIO) -> str:
    starting = mapping_file_fp.tell()
    try:
        mapping_file_fp.seek(0)
        mapping_data = tomllib.load(mapping_file_fp)
        try:
            return mapping_data["mappings"]["identifier_key"]
        except KeyError as e:
            raise BadMappingDataError(
                "Mapping file does not contain 'identifier_key' in the 'mappings' section"
            ) from e
    finally:
        mapping_file_fp.seek(starting)


def get_identifier_key(
    mapping_file: pathlib.Path,
    strategy: Callable[[BinaryIO], str] = get_identifier_key_fp,
) -> str:
    with mapping_file.open("rb") as f:
        return strategy(f)


def _get_new_data_from_marc(
    mapped_value: str, record: ET.Element
) -> List[str]:
    ns = {"marc": "http://www.loc.gov/MARC21/slim"}
    marc_re = re.compile(MARC_REGEX)
    re_results = marc_re.search(mapped_value)
    if not re_results:
        return []
    mapped_value_results = re_results.groupdict()
    data_field = mapped_value_results.get("datafield")
    if not data_field:
        raise ValueError(
            f'failed to parse matching key "{mapped_value}" when mapping '
        )
    sub_field = mapped_value_results.get("subfield")
    xpath = get_xpath(data_field, sub_field, "marc")
    new_data = []
    for res in record.findall(xpath, ns):
        if not res.text:
            continue
        try:
            data = res.text.strip()
        except AttributeError as e:
            raise ValueError(
                f'failed to get result from matching key "{mapped_value}"'
            ) from e
        if not data:
            continue
        new_data.append(data)
    return new_data


def locate_marc_value_in_record(
    config: MappingConfig, record: ET.Element
) -> Optional[str]:
    new_data: List[str] = []
    for mapped_value in config.matching_keys:
        new_data += _get_new_data_from_marc(mapped_value, record)
    if len(new_data) == 0:
        return None
    return config.delimiter.join(new_data)


def merge_data_from_getmarc(
    mapping_file_fp: BinaryIO,
    input_metadata_tsv_fp: TextIO,
    get_marc_server_strategy: Callable[[str], ET.Element],
    dialect: Union[Type[csv.Dialect], csv.Dialect],
) -> List[Dict[str, str]]:
    mapping = read_mapping_toml_data(
        mapping_file_fp, map_marc_mapping_to_mapping_config
    )
    identifier_key: str = get_identifier_key_fp(mapping_file_fp)

    new_rows: List[Dict[str, str]] = []

    def _iter_row(
        fp: TextIO, _dialect: Union[Type[csv.Dialect], csv.Dialect, str]
    ) -> Iterable[TableRow[Dict[str, str]]]:
        yield from tsv.iter_tsv_fp(fp, dialect=_dialect)

    for row in _iter_row(input_metadata_tsv_fp, dialect):
        record = get_marc_server_strategy(row.entry[identifier_key])
        merged_row = row.entry.copy()

        for mapped_source_key, mapping_configuration in mapping.items():
            # Optimization: if there is already data and existing_data is
            # set to ignored anyway, skip this key and move on to the next
            # one
            if (
                merged_row[mapped_source_key].strip()
                and mapping_configuration.existing_data == "keep"
            ):
                logger.debug(
                    'Use existing value for "%s" on line %d',
                    mapped_source_key,
                    row.line_number,
                )
                continue

            if new_value := locate_marc_value_in_record(
                mapping_configuration, record
            ):
                if not merged_row[mapped_source_key].strip():
                    merged_row[mapped_source_key] = new_value
                    logger.info(
                        'Setting "%s" to "%s" on line %d',
                        mapped_source_key,
                        new_value,
                        row.line_number,
                    )
                    continue

                match mapping_configuration.existing_data:
                    case "replace":
                        logger.info(
                            'Overwriting existing value for "%s"',
                            mapped_source_key,
                        )
                        merged_row[mapped_source_key] = new_value

                    case "append":
                        logger.info(
                            'Appending existing value for "%s"',
                            mapped_source_key,
                        )
                        merged_row[mapped_source_key] = (
                            f"{merged_row[mapped_source_key]}{mapping_configuration.delimiter}{new_value}"
                        )

                    case _:
                        raise ValueError(
                            f"Unknown value for existing_data: {mapping_configuration.existing_data}"
                        )
        new_rows.append(merged_row)
    return new_rows


def write_new_rows_to_file(
    rows: List[Dict[str, str]],
    dialect: Union[Type[csv.Dialect], csv.Dialect],
    fp: TextIO,
) -> None:
    writer = csv.DictWriter(fp, fieldnames=rows[0].keys(), dialect=dialect)
    writer.writeheader()
    for row in rows:
        writer.writerow(row)


def merge_from_getmarc(
    input_metadata_tsv_file: pathlib.Path,
    output_metadata_tsv_file: pathlib.Path,
    mapping_file: pathlib.Path,
    get_marc_server: str,
    row_merge_data_strategy: Callable[
        [
            BinaryIO,
            TextIO,
            Callable[[str], ET.Element],
            Union[Type[csv.Dialect], csv.Dialect],
        ],
        List[Dict[str, str]],
    ] = merge_data_from_getmarc,
    write_to_file_strategy: Callable[
        [List[Dict[str, str]], Union[Type[csv.Dialect], csv.Dialect], TextIO],
        None,
    ] = write_new_rows_to_file,
) -> None:
    """Merge data from GetMARC server into a TSV file using a mapping file.

    Args:
        input_metadata_tsv_file: Source TSV file to be merged with.
        output_metadata_tsv_file: Output TSV file to be created or overwritten.
        mapping_file: Mapping file to be used for merging.
        get_marc_server: url of the GetMARC server.
        row_merge_data_strategy: strategy to create new rows from GetMARC
            server and input tsv file.
        write_to_file_strategy: strategy to write new rows to the output file.

    """
    try:
        with input_metadata_tsv_file.open("r") as input_metadata_tsv_file_fp:
            dialect = tsv.get_tsv_dialect(input_metadata_tsv_file_fp)
            with mapping_file.open("rb") as mapping_file_fp:
                new_rows = row_merge_data_strategy(
                    mapping_file_fp,
                    input_metadata_tsv_file_fp,
                    functools.partial(
                        get_matching_marc_data, get_marc_server=get_marc_server
                    ),
                    dialect,
                )
    except BadMappingDataError as mapping_data_error:
        raise BadMappingFileError(
            source_file=mapping_file, details=mapping_data_error.details
        ) from mapping_data_error

    with output_metadata_tsv_file.open("w") as f:
        write_to_file_strategy(new_rows, dialect, f)
