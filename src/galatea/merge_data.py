"""Merge data from one source with another.

Added in version 0.4.0.

"""

import collections
import functools
import logging
import pathlib
import re
import csv
import sys
import warnings
import xml

import jinja2

if sys.version_info >= (3, 11):
    import tomllib
else:
    import tomli as tomllib
import typing
from typing import (
    List,
    Dict,
    TextIO,
    Callable,
    BinaryIO,
    Type,
    Union,
    Optional,
    Iterable,
)
import dataclasses
import requests
from xml.etree import ElementTree as ET

from galatea import tsv
from galatea.tsv import TableRow
from galatea.utils import GalateaException, CommandFinishedWithException


__all__ = [
    "generate_mapping_file_for_tsv",
    "merge_from_getmarc",
    "BadMappingFileError",
]

MARC_SLIM_XML_NAMESPACE = "http://www.loc.gov/MARC21/slim"  # NOSONAR

MARC_RECORD = ET.Element

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class ExperimentalFeatureError(GalateaException):
    def __init__(self, *args, source: str) -> None:
        super().__init__(*args)
        self.source = source


class BadMappingDataError(GalateaException):
    """Exception raised when a mapping data is malformed or invalid."""

    def __init__(self, *args, details: Optional[str] = None) -> None:
        super().__init__(*args)
        self.details = details


class BadMappingFileError(GalateaException):
    """Exception raised when a mapping data in a file malformed or invalid."""

    def __init__(
        self, source_file: pathlib.Path, *args, details: Optional[str] = None
    ) -> None:
        """Initialize a new BadMappingFileError exception.

        Args:
            source_file: source file containing the error
            *args: standard exception args
            details: any details about error
        """
        super().__init__(*args)
        self.source = source_file
        self.details = details

    def __str__(self) -> str:
        """Print string."""
        if self.details:
            return (
                f"Issue with mapping file: {self.source}. "
                f"Details: {self.details}."
            )
        return f"Issue with mapping file: {self.source}"


class SerialzationError(GalateaException): ...


class GetMarcRetrievalError(GalateaException):
    """Unable to access record from GetMarc Server."""

    def __init__(self, mmsid=None, *args):
        super().__init__(*args)
        self.mmsid = mmsid

    def __str__(self):
        if self.mmsid:
            return f'GetMarcRetrievalError: Unable to get record "{self.mmsid}" record from getmarc server.'
        return "GetMarcRetrievalError"


serialization_methods = {"verbatim": None}


@dataclasses.dataclass
class MappingConfig:
    key: str
    matching_keys: List[str]
    delimiter: str
    existing_data: str
    serialize_method: str = "verbatim"
    experimental: Dict[str, Dict[str, Union[str, List[str]]]] = (
        dataclasses.field(default_factory=dict)
    )


def get_keys_from_tsv_fp(fp: TextIO) -> List[str]:
    """Get the keys from a TSV file.

    Args:
        fp: file pointer to the TSV file.

    Returns: list of keys (headings) in the TSV file.

    """

    def _iter_keys_from_fp(fp: TextIO) -> TableRow[Dict[str, str]]:
        return next(iter(tsv.iter_tsv_fp(fp, dialect="excel-tab")))

    headings = []
    starting = fp.tell()
    try:
        fp.seek(0)
        for key in _iter_keys_from_fp(fp).entry.keys():
            if not key:
                continue
            if key in headings:
                continue
            headings.append(key)
        return headings
    finally:
        fp.seek(starting)


def get_keys_from_tsv(
    tsv_file: pathlib.Path,
    strategy: Callable[[TextIO], List[str]] = get_keys_from_tsv_fp,
) -> List[str]:
    with tsv_file.open("r", encoding="utf-8") as f:
        return strategy(f)


def generate_mapping_toml_file_for_tsv_fp(
    source_file_name: str, headings: List[str], fp: TextIO
) -> None:
    """Generate a mapping file for a TSV file.

    Args:
        source_file_name: source file name, used in the mapping file
        headings: list of headings in the TSV file.
        fp: file pointer to write the mapping file to.

    """
    boilerplate = f"""# ====================================================
# Mapping file for "{source_file_name}"
# ====================================================

# This config file is generated by galatea to help you map data from another
# source (such as GetMARC/ALMA) into the fields of your tsv file.

[mappings]

# the value set in identifier_key is the key that use look up a matching record
identifier_key = "Bibliographic Identifier"

# Within this [mappings] section, there is a list of sub-elements
# entitled [[mapping]]. This is where you do your mapping. The key
# corresponds to the key in the tsv file, and the matching_keys is a list of
# keys that you want to pull data from.

# For example. If you want to pull data from the 264$c field of the MARC
# record into the "Date" column, your mapping file would look like this:

# [[mapping]]
# key = "Date"
# matching_marc_fields = [
#   "264$c"
# ]

# This will map the 264$c field of the MARC record into the "Date" column of
# your tsv file.

# If you do not want to map a field, you can leave the matching_keys empty or
# completely remove the [[mapping]] section.

"""
    fp.write(boilerplate)
    for i, key in enumerate(headings):
        fp.write("[[mapping]]\n")
        fp.write(f'key = "{key}"\n')
        fp.write("matching_marc_fields = []\n")
        fp.write('delimiter = "||"\n')
        fp.write('existing_data = "keep"\n')
        fp.write("\n")
    fp.write(
        """
# This file uses the TOML format. If you should need more information about the
# syntax, see: https://toml.io/en/
""".lstrip()
    )


def generate_mapping_file_for_tsv(
    tsv_file: pathlib.Path,
    output_file: pathlib.Path,
    headers_reading_strategy: Callable[
        [pathlib.Path], List[str]
    ] = get_keys_from_tsv,
    writing_strategy: Callable[
        [str, List[str], TextIO], None
    ] = generate_mapping_toml_file_for_tsv_fp,
) -> None:
    """Generate a mapping file for a TSV file.

    The mapping file will contain the field names and what index they are
    mapped from
    """
    with output_file.open("w", encoding="utf-8") as f:
        writing_strategy(tsv_file.name, headers_reading_strategy(tsv_file), f)
    print(f"Wrote mapping file to {output_file.absolute()}")


def get_matching_marc_data(
    mmsid: str,
    get_marc_server: str,
    request_strategy: Callable[[str], requests.Response] = requests.get,
) -> ET.Element:
    result = request_strategy(f"{get_marc_server}/api/record?mms_id={mmsid}")
    try:
        return ET.fromstring(result.text)
    except xml.etree.ElementTree.ParseError as e:
        raise GetMarcRetrievalError(mmsid=mmsid) from e


MARC_REGEX = r"^(?P<datafield>\d{3})((\$?(?P<subfield>[a-zA-Z0-9]{1}))?$)"


def get_xpath(datafield: str, subfield: Union[str, None], prefix: str) -> str:
    if not subfield:
        return f".//{prefix}:datafield[@tag='{datafield}']"
    return f".//{prefix}:datafield[@tag='{datafield}']/{prefix}:subfield[@code='{subfield}']"  # noqa: E501


# ===================================================================
# Validation functions for mapping entries


def validate_is_list_of_strings(
    entry: Dict[str, Union[str, List[str]]], key: str
) -> Optional[str]:
    value = entry.get(key)
    if not isinstance(value, list):
        return f"'{key}' should be a list of strings"

    for item in value:
        if not isinstance(item, str):
            return (
                f"'{key}' should be a list of strings, but found "
                f"{type(item).__name__}"
            )
    return None


def validate_is_not_list(
    entry: Dict[str, Union[str, List[str]]], key: str
) -> Optional[str]:
    value = entry.get(key)
    if isinstance(value, list):
        return f"'{key}' should be a string, not a list"
    return None


def validate_is_string(
    entry: Dict[str, Union[str, List[str]]], key: str
) -> Optional[str]:
    value = entry.get(key)
    if not isinstance(value, str):
        return f"'{key}' should be a string"
    return None


def validate_limited_to_values(
    entry: Dict[str, Union[str, List[str]]],
    key: str,
    allowed_values: List[str],
) -> Optional[str]:
    value = entry.get(key)
    if value not in allowed_values:
        return (
            f"'{key}' should be one of {allowed_values}, but found '{value}'"
        )
    return None


# ===================================================================
def matching_marc_fields(
    entry: Dict[str, Union[str, List[str]]],
) -> Optional[str]:
    if (
        "serialize_method" not in entry
        or entry["serialize_method"] == "verbatim"
    ):
        return validate_is_list_of_strings(entry, key="matching_marc_fields")
    return None


DEFAULT_MARC_MAPPING_VALIDATIONS: List[
    Callable[[Dict[str, Union[str, List[str]]]], Optional[str]]
] = [
    functools.partial(validate_is_not_list, key="key"),
    matching_marc_fields,
    functools.partial(validate_is_string, key="delimiter"),
    functools.partial(
        validate_limited_to_values,
        key="existing_data",
        allowed_values=["keep", "replace", "append"],
    ),
]


def get_experimental_values(
    entry,
) -> Dict[str, Dict[str, Union[str, List[str]]]]:
    if "serialize_method" in entry:
        if entry["serialize_method"] == "jinja2template":
            return {"jinja2template": {"template": entry["jinja_template"]}}
    return {}


def map_marc_mapping_to_mapping_config(
    entry: Dict[str, Union[str, List[str]]],
    validations: Optional[
        List[Callable[[Dict[str, Union[str, List[str]]]], Optional[str]]]
    ] = None,
) -> MappingConfig:
    errors: List[str] = []
    for check in validations or DEFAULT_MARC_MAPPING_VALIDATIONS:
        found_issue = check(entry)
        if found_issue:
            errors.append(found_issue)

    if errors:
        raise BadMappingDataError(
            "Malformed mapping file: " + ", ".join(errors)
        )
    return MappingConfig(
        key=typing.cast(str, entry["key"]),
        matching_keys=(
            typing.cast(List[str], entry.get("matching_marc_fields", []))
        ),
        serialize_method=typing.cast(
            str, entry.get("serialize_method", "verbatim")
        ),
        delimiter=typing.cast(str, entry.get("delimiter", "||")),
        existing_data=typing.cast(str, entry.get("existing_data", "keep")),
        experimental=get_experimental_values(entry),
    )


def read_mapping_toml_data(
    mapping_file_fp: BinaryIO,
    mapping_to_config_strategy: Optional[
        Callable[[Dict[str, Union[str, List[str]]]], MappingConfig]
    ] = None,
    _: bool = False,
) -> Dict[str, MappingConfig]:
    starting = mapping_file_fp.tell()
    mapping = {}
    try:
        mapping_file_fp.seek(0)
        mapping_data = tomllib.load(mapping_file_fp)
        for mapping_value in mapping_data["mapping"]:
            try:
                if mapping_to_config_strategy is None:
                    mapping[mapping_value["key"]] = MappingConfig(
                        **mapping_value
                    )
                else:
                    mapping[mapping_value["key"]] = mapping_to_config_strategy(
                        mapping_value,
                    )
            except TypeError:
                raise BadMappingDataError("Malformed mapping file")
        return mapping
    except tomllib.TOMLDecodeError as toml_error:
        raise BadMappingDataError(details=str(toml_error)) from toml_error
    finally:
        mapping_file_fp.seek(starting)


def get_identifier_key_fp(mapping_file_fp: BinaryIO) -> str:
    starting = mapping_file_fp.tell()
    try:
        mapping_file_fp.seek(0)
        mapping_data = tomllib.load(mapping_file_fp)
        try:
            return mapping_data["mappings"]["identifier_key"]
        except KeyError as e:
            raise BadMappingDataError(
                "Mapping file does not contain 'identifier_key' in the "
                "'mappings' section"
            ) from e
    finally:
        mapping_file_fp.seek(starting)


def get_identifier_key(
    mapping_file: pathlib.Path,
    strategy: Callable[[BinaryIO], str] = get_identifier_key_fp,
) -> str:
    with mapping_file.open("rb") as f:
        return strategy(f)


def _get_new_data_from_marc(
    mapped_value: str, record: ET.Element
) -> List[str]:
    ns = {"marc": MARC_SLIM_XML_NAMESPACE}
    marc_re = re.compile(MARC_REGEX)
    re_results = marc_re.search(mapped_value)
    if not re_results:
        return []
    mapped_value_results = re_results.groupdict()
    data_field = mapped_value_results.get("datafield")
    if not data_field:
        raise ValueError(
            f'failed to parse matching key "{mapped_value}" when mapping '
        )
    sub_field = mapped_value_results.get("subfield")
    xpath = get_xpath(data_field, sub_field, "marc")
    new_data = []
    for res in record.findall(xpath, ns):
        if not res.text:
            continue
        try:
            data = res.text.strip()
        except AttributeError as e:
            raise ValueError(
                f'failed to get result from matching key "{mapped_value}"'
            ) from e
        if not data:
            continue
        new_data.append(data)
    return new_data


def locate_marc_value_in_record(
    config: MappingConfig, record: ET.Element
) -> Optional[str]:
    new_data: List[str] = []
    for mapped_value in config.matching_keys:
        new_data += _get_new_data_from_marc(mapped_value, record)
    if len(new_data) == 0:
        return None
    return config.delimiter.join(new_data)


def experimental_feature(func):
    def inner(*args, **kwargs):
        if "enable_experimental_features" in kwargs:
            if kwargs["enable_experimental_features"] is False:
                raise ExperimentalFeatureError(
                    f'"{func.__name__}" is an experimental feature.',
                    source=func.__name__,
                )
            else:
                warnings.warn(
                    "Using an experimental feature. Notice that this "
                    "could change at any time",
                    UserWarning,
                )
        return func(*args, **kwargs)

    return inner


def organize_marc_one_code_per_subfield(marc_record: ET.Element):
    fields = collections.defaultdict(list)
    ns = {"marc": MARC_SLIM_XML_NAMESPACE}
    for res in marc_record.findall(".//marc:datafield", ns):
        subfield_data = {}
        for sub_field in res.findall(".//marc:subfield", ns):
            subfield_data[sub_field.attrib["code"]] = sub_field.text
        fields[res.attrib["tag"]].append(subfield_data)
    return fields


def organize_with_code_and_value(marc_record: ET.Element):
    fields = collections.defaultdict(list)
    ns = {"marc": MARC_SLIM_XML_NAMESPACE}
    for res in marc_record.findall(".//marc:datafield", ns):
        subfield_data = []
        for sub_field in res.findall(".//marc:subfield", ns):
            subfield_data.append({
                "code": sub_field.attrib["code"],
                "value": sub_field.text,
            })
        fields[res.attrib["tag"]].append(subfield_data)
    return fields


@experimental_feature
def serialize_with_jinja_template(
    marc_record: ET.Element,
    config: MappingConfig,
    enable_experimental_features: bool,
) -> str:
    serialization_method = config.experimental[config.serialize_method]
    jinja_template = "".join(
        typing.cast(str, serialization_method["template"]).split("\n")
    )
    template = jinja2.Template(jinja_template)
    fields = organize_with_code_and_value(marc_record)
    try:
        return template.render(fields=fields)
    except TypeError as error:
        raise SerialzationError(
            f"Unable to render Jinja template due to an issue related to types "
            f"used in the template. {error}"
        ) from error


class MergeRowData:
    def __init__(self, marc_record: MARC_RECORD) -> None:
        self.marc_record = marc_record
        self.serialize_value_strategy: Callable[
            [MARC_RECORD, MappingConfig, bool], Optional[str]
        ] = lambda _marc_record, config, _: locate_marc_value_in_record(
            config, _marc_record
        )
        self.enable_experimental_features: bool = False

    def merge_row_data(
        self,
        mapped_source_key: str,
        row: Dict[str, str],
        mapping_configuration: MappingConfig,
        line_number: int,
    ) -> None:
        if (
            row[mapped_source_key]
            and mapping_configuration.existing_data == "keep"
        ):
            logger.debug(
                'Use existing value for "%s" on line %d',
                mapped_source_key,
                line_number,
            )
            return
        if serialized_value := self.serialize_value_strategy(
            self.marc_record,
            mapping_configuration,
            self.enable_experimental_features,
        ):
            if not row[mapped_source_key]:
                row[mapped_source_key] = serialized_value
                logger.debug(
                    'Setting "%s" to "%s" on line %d',
                    mapped_source_key,
                    serialized_value,
                    line_number,
                )
                return

            match mapping_configuration.existing_data:
                case "replace":
                    logger.debug(
                        'Overwriting existing value for "%s"',
                        mapped_source_key,
                    )
                    row[mapped_source_key] = serialized_value

                case "append":
                    logger.debug(
                        'Appending existing value for "%s"',
                        mapped_source_key,
                    )
                    delim = mapping_configuration.delimiter
                    row[mapped_source_key] = (
                        f"{row[mapped_source_key]}{delim}{serialized_value}"
                    )

                case _:
                    raise ValueError(
                        "Unknown value for existing_data: "
                        f"{mapping_configuration.existing_data}"
                    )


def serialization_base_on_config(
    record: ET.Element,
    config: MappingConfig,
    enable_experimental_features: bool,
) -> Optional[str]:
    match config.serialize_method:
        case "verbatim":
            return locate_marc_value_in_record(config, record)
        case "jinja2template":
            return serialize_with_jinja_template(
                record,
                config,
                enable_experimental_features=enable_experimental_features,
            )
    return None


class NonFatalMergingRowError(GalateaException):
    """Error but that should be logged without causing termination."""

    def __init__(
        self, message: Optional[str] = None, recovered_data=None, *args
    ):
        self.recovered_data = recovered_data
        self.message = message
        super().__init__(*args)

    def __str__(self):
        if self.message:
            return self.message
        return "NonFatalMergingRowError"


def is_row_empty(
    row: Dict[str, str],
) -> bool:
    return all(not str(e).strip() for e in row.values())


def merge_data_from_getmarc(
    mapping_file_fp: BinaryIO,
    input_metadata_tsv_fp: TextIO,
    get_marc_server_strategy: Callable[[str], ET.Element],
    dialect: Union[Type[csv.Dialect], csv.Dialect],
    enable_experimental_features: bool = False,
) -> List[Dict[str, Union[str, str]]]:
    non_fatal_errors = []
    mapping = read_mapping_toml_data(
        mapping_file_fp,
        map_marc_mapping_to_mapping_config,
        enable_experimental_features,
    )
    identifier_key: str = get_identifier_key_fp(mapping_file_fp)

    new_rows: List[Dict[str, str]] = []

    def _iter_row(
        fp: TextIO, _dialect: Union[Type[csv.Dialect], csv.Dialect, str]
    ) -> Iterable[TableRow[Dict[str, str]]]:
        yield from tsv.iter_tsv_fp(fp, dialect=_dialect)

    warned_extra_keys = set()
    for row in _iter_row(input_metadata_tsv_fp, dialect):
        try:
            if is_row_empty(row.entry):
                logger.warning("Row #%s is empty", row.line_number)
                new_rows.append(row.entry)
                continue
            logger.info("Mapping row #%s.", row.line_number)
            try:
                record = get_marc_server_strategy(
                    typing.cast(str, row.entry[identifier_key])
                )
            except GetMarcRetrievalError as e:
                logger.error(
                    "Unable to access marc information from row #%s. Reason: %s",
                    row.line_number,
                    e,
                )
                new_rows.append(row.entry)
                raise NonFatalMergingRowError(
                    f"Unable to merge data from row #{row.line_number}"
                )

            merged_row: Dict[str, str] = row.entry.copy()
            merger = MergeRowData(record)
            merger.serialize_value_strategy = serialization_base_on_config
            merger.enable_experimental_features = enable_experimental_features
            for mapped_source_key, mapping_configuration in mapping.items():
                # Don't fail if the mapper contains extra keys, just warn the user
                # about it once.
                if mapped_source_key not in row.entry:
                    if mapped_source_key not in warned_extra_keys:
                        warned_extra_keys.add(mapped_source_key)
                        logger.warning(
                            'Mapping contains key not found in table: "%s"',
                            mapped_source_key,
                        )
                    continue

                # Optimization: if there is already data and existing_data is
                # set to ignored anyway, skip this key and move on to the next
                # one
                try:
                    merger.merge_row_data(
                        mapped_source_key,
                        merged_row,
                        mapping_configuration,
                        row.line_number,
                    )
                except SerialzationError as e:
                    raise SerialzationError(
                        f'Tried to serialize line {row.line_number}, column "{mapped_source_key}" of tsv file. {str(e)}'
                    ) from e
            new_rows.append(merged_row)
        except NonFatalMergingRowError as e:
            non_fatal_errors.append(str(e))

    if non_fatal_errors:
        errors_list = "\n".join([f"* {e}" for e in non_fatal_errors])
        logger.error(
            f"The following error(s) occured while trying to merge data.\n{errors_list}"
        )
        raise NonFatalMergingRowError(recovered_data=new_rows)
    return new_rows


def write_new_rows_to_file(
    rows: List[Dict[str, str]],
    dialect: Union[Type[csv.Dialect], csv.Dialect],
    fp: TextIO,
    dict_writer: Type[csv.DictWriter] = csv.DictWriter,
) -> None:
    field_names = rows[0].keys()
    # Just make sure that every row has the same keys
    if all([r.keys() == rows[0].keys() for r in rows]) is False:
        raise ValueError("Not all row have the same keys")

    writer = dict_writer(fp, fieldnames=field_names, dialect=dialect)
    writer.writeheader()
    for row in rows:
        try:
            writer.writerow(row)
        except csv.Error as e:
            logger.error(f"Failed to write: {row}")
            raise GalateaException("Unable to write row") from e


def merge_from_getmarc(
    input_metadata_tsv_file: pathlib.Path,
    output_metadata_tsv_file: pathlib.Path,
    mapping_file: pathlib.Path,
    get_marc_server: str,
    row_merge_data_strategy: Callable[
        [
            BinaryIO,
            TextIO,
            Callable[[str], ET.Element],
            Union[Type[csv.Dialect], csv.Dialect],
            bool,
        ],
        List[Dict[str, str]],
    ] = merge_data_from_getmarc,
    write_to_file_strategy: Callable[
        [List[Dict[str, str]], Union[Type[csv.Dialect], csv.Dialect], TextIO],
        None,
    ] = write_new_rows_to_file,
    enable_experimental_features: bool = False,
) -> None:
    """Merge data from GetMARC server into a TSV file using a mapping file.

    Args:
        input_metadata_tsv_file: Source TSV file to be merged with.
        output_metadata_tsv_file: Output TSV file to be created or overwritten.
        mapping_file: Mapping file to be used for merging.
        get_marc_server: url of the GetMARC server.
        row_merge_data_strategy: strategy to create new rows from GetMARC
            server and input tsv file.
        write_to_file_strategy: strategy to write new rows to the output file.
        enable_experimental_features: enable experimental features that are not

    """
    successful_with_no_issues = True
    try:
        with input_metadata_tsv_file.open(
            "r", encoding="utf-8"
        ) as input_metadata_tsv_file_fp:
            dialect = tsv.get_tsv_dialect(input_metadata_tsv_file_fp)
            with mapping_file.open("rb") as mapping_file_fp:
                try:
                    new_rows = row_merge_data_strategy(
                        mapping_file_fp,
                        input_metadata_tsv_file_fp,
                        functools.partial(
                            get_matching_marc_data,
                            get_marc_server=get_marc_server,
                        ),
                        dialect,
                        enable_experimental_features,
                    )
                except NonFatalMergingRowError as e:
                    new_rows = e.recovered_data
                    successful_with_no_issues = False
    except BadMappingDataError as mapping_data_error:
        raise BadMappingFileError(
            source_file=mapping_file, details=mapping_data_error.details
        ) from mapping_data_error

    with output_metadata_tsv_file.open("w", encoding="utf-8") as f:
        write_to_file_strategy(new_rows, dialect, f)

    if not successful_with_no_issues:
        raise CommandFinishedWithException(
            f"Unable to complete merge data from GetMARC. "
            f"{output_metadata_tsv_file} was written to the best it could."
        )
